---
title: 'Intro to TM Badge'
subtitle: "LASER Institute TM Learning Lab 1"
author: "Kim Wright"
date: "`r format(Sys.Date(),'%B %e, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

![](img/tmb.png){width="300"}

The final activity for each learning lab provides space to work with data and to reflect on how the concepts and techniques introduced in each lab might apply to your own research.

To earn a badge for each lab, you are required to respond to a set of prompts for two parts: 

-   In Part I, you will reflect on your understanding of key concepts and begin to think about potential next steps for your own study.

-   In Part II, you will create a simple data product in R that demonstrates your ability to apply a data analysis technique introduced in this learning lab.

### Part I: Reflect and Plan

Use the institutional library (e.g. [NCSU Library](https://www.lib.ncsu.edu/#articles)), [Google Scholar](https://scholar.google.com/) or search engine to locate a research article, presentation, or resource that applies text mining to an educational context or topic of interest. More specifically, **locate a text mining study that visualize text data.**

1.  Provide an APA citation for your selected study.

    -   Lai, R.P.Y. (2021). Teachers’ ontological perspectives of computational thinking and assessment: A text mining approach. *Journal of Educational Computing Research, 60*(3), 661-695

2.  How does the visualization address research questions?

    -   The researcher used created a bar graph and word clouds of teachers' views of CT using term frequency analysis, word clouds, and word association.

Draft a research question for a population you may be interested in studying, or that would be of interest to educational researchers, and that would require the collection of text data and answer the following questions: 

**Kim's research questions for text mining:   
(1) What are school districts' views on moving to 100% online state testing?  
(2) What additional resources do schools identify as necessary in order to transition to online state testing?   
(3) Are there differences in districts' perceptions of resources needed to transition to online testing?**

1.  What text data would need to be collected?

    -   I have data from a survey my center conducted with our state's school districts in spring 2020. The data file has the open ended questions about resources for readiness, as well as a host of quant questions and district identifiers that we merged in from publicly available state data describing districts (e.g., grades served, number of students, academic performance data, etc.) 

2.  For what reason would text data need to be collected in order to address this question?

    -   Text data can provide the districts' perceptions about shifting to 100% online state testing. The report we wrote at the time focused on the costs of shifting, but analysis of the open-ended text data can really tell the story of how districts feel(felt) about the shift. I personally feel that this is an aspect of education policy implementation that frequently goes untold.

3.  Explain the analytical level at which these text data would need to be collected and analyzed.

    -   The primary unit of analysis is the school district- the surveys were completed by district-level personnel and would be analyzed at the district level. There would be no need to aggregate the data up to another level for the current study.

### Part II: Data Product

Use your case study file to create a new word cloud that does not include words that would give you important information about teachers' experiences with professional development. (For example, we did not include "University" in the word cloud describing where scholar came from as it occurs everywhere).

I highly recommend creating a new R script in your lab-1 folder to complete this task. When your code is ready to share, use the code chunk below to share the final code for your model and answer the questions that follow.

```{r, my-data-product}
# Kim's FINAL CODE HERE
######PREPARE###########

#loading packages
library(tidyverse)
library(tidytext)


######WRANGLE##########

#reading data into r
opd_survey <- read_csv("data/opd_survey.csv")

#viewing data 
view(opd_survey)

#reducing data to a few variables including Q21 most beneficial/valuable aspect
opd_selected <- select(opd_survey, Role, Resource...6, Q21)

#remaining Q21 to meaningful variable name
opd_renamed <- rename(opd_selected, Resource = Resource...6, valueofpd = Q21)

#eliminating crazy Qualtrics rows that ALWAYS get in the way
#"-" sign indicates to drop rows 1 and 2
opd_sliced <- slice(opd_renamed, -1, -2)

#omitting cases with missing data 
opd_complete <- na.omit(opd_sliced)

#filtering data for cases where Role==Teacher
opd_teacher <- filter(opd_complete, Role == "Teacher")

#unnesting data into tokens
opd_tidy <- unnest_tokens(opd_teacher, word, valueofpd)

#removing stop words
opd_clean <- anti_join(opd_tidy, stop_words)


######EXPLORE########

#loading dplyr package
library(dplyr)

#word counts for valueofpd
opd_counts <- count(opd_clean, word, sort = TRUE)
opd_counts

#removing unimportant words
unimportant <- data.frame("word" = c("beneficial", "easy", "understand", "provided", "helpful", "aspect", "valuable", "enjoyed", "easily","convenient", "importance","line"))
opd_clean2 <- anti_join(opd_counts, unimportant)

#loading wordcloud library
library(wordcloud2)

#creating wordcloud for opd_clean2
wordcloud2(opd_clean2)


```

### Knit & Submit

Congratulations, you've completed your Intro to text mining Badge! Complete the following steps to submit your work for review:

1.  Change the name of the `author:` in the [YAML header](https://monashdatafluency.github.io/r-rep-res/yaml-header.html) at the very top of this document to your name. As noted in [Reproducible Research in R](https://monashdatafluency.github.io/r-rep-res/index.html), The YAML header controls the style and feel for knitted document but doesn't actually display in the final output.

2.  Click the yarn icon above to "knit" your data product to a [HTML](https://bookdown.org/yihui/rmarkdown/html-document.html) file that will be saved in your R Project folder.

3.  Commit your changes in GitHub Desktop and push them to your online GitHub repository.

4.  Publish your HTML page the web using one of the following [publishing methods](https://rpubs.com/cathydatascience/518692):

    -   Publish on [RPubs](https://rpubs.com) by clicking the "Publish" button located in the Viewer Pane when you knit your document. Note, you will need to quickly create a RPubs account.

    -   Publishing on GitHub using either [GitHub Pages](https://pages.github.com) or the [HTML previewer](http://htmlpreview.github.io).

5.  Post a new discussion on GitHub to our [Text mining Badges forum](https://github.com/orgs/laser-institute/teams/network-analysis/discussions/3). In your post, include a link to your published web page and a short reflection highlighting one thing you learned from this lab and one thing you'd like to explore further.
